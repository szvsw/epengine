"""Simulate an EnergyPlus ubem shoebox model with associated artifacts."""

import asyncio
import hashlib
import logging

import numpy as np
import pandas as pd
from hatchet_sdk import Context

from epengine.hatchet import hatchet
from epengine.models.mixins import WithHContext
from epengine.models.shoebox_sbem import SBEMSimulationSpec
from epengine.utils.results import serialize_df_dict

logger = logging.getLogger(__name__)


class SBEMSimulationSpecWithContext(WithHContext, SBEMSimulationSpec):
    """A simulation specification with a Hatchet Context."""

    pass


# TODO: This could be generated by a class method in the SimulationSpec class
# but should it?
@hatchet.workflow(
    name="simulate_sbem_shoebox",
    timeout="10m",
    version="0.3",
    schedule_timeout="1000m",
)
class SimulateSBEMShoebox:
    """A workflow to simulate an EnergyPlus model."""

    @hatchet.step(name="simulate", timeout="10m", retries=2)
    async def simulate(self, context: Context):
        """Simulate an EnergyPlus Shoebox UBEM model.

        Args:
            context (Context): The context of the workflow

        Returns:
            dict: A dictionary of dataframes with results.
        """

        def run():
            data = context.workflow_input()
            spec = SBEMSimulationSpecWithContext(**data, hcontext=context)
            _idf, results, err_text = spec.run(log_fn=context.log)
            context.log(err_text)

            # features = spec.feature_dict
            # features["experiment_id"] = spec.experiment_id
            # features["sort_index"] = spec.sort_index
            # results = toy_results(features)

            results = {"results": results}
            return serialize_df_dict(results)

        return await asyncio.to_thread(run)


def toy_results(feature_dict: dict[str, float | int | str]):
    """Toy results for testing."""
    features_as_multiindex = pd.MultiIndex.from_frame(
        pd.DataFrame(feature_dict, index=pd.Index([0]))
    )
    results = pd.DataFrame(
        {"output_1": [0.0], "output_2": [0.0], "output_3": [0.0]},
        index=features_as_multiindex,
    )
    index = results.index.to_frame(index=False)
    index = index[[col for col in index.columns if col.startswith("feature")]]
    numericals = index.select_dtypes(include=[float, int]).values.flatten()
    numericals_squared = (numericals * numericals.reshape(-1, 1)).flatten()
    categoricals = index.select_dtypes(include="object").values.flatten()

    categoricals_hashed = np.array([
        int(hashlib.sha256(cat.encode()).hexdigest()[-2:], 16) for cat in categoricals
    ])

    indicators = np.concatenate([numericals, numericals_squared, categoricals_hashed])
    seed = 42
    gen = np.random.RandomState(seed)
    results_cols = results.columns.to_list()
    coeff_mat = gen.uniform(size=(len(results_cols), len(indicators)))
    linear_terms = coeff_mat @ indicators
    for i, col in enumerate(results_cols):
        results.loc[:, col] = linear_terms[i]
    return results


if __name__ == "__main__":
    import datetime
    from pathlib import Path

    import boto3
    from hatchet_sdk import new_client
    from hatchet_sdk.clients.admin import WorkflowRunDict
    from pydantic import AnyUrl

    n_sims = 100  # 4500 * 8
    experiment_id = "test/shoebox-sbem-benchmark-4"
    bucket = "ml-for-bem"
    bucket_prefix = "hatchet"
    artifact_path = Path("E:/repos/epinterface/tests/data")
    semantic_fields_local_path = artifact_path / "semantic-fields-ma-with-abs.yml"
    component_map_local_path = artifact_path / "component-map-ma-with-abs.yml"
    db_local_path = artifact_path / "components-ma-with-abs.db"
    epwzip_path = "https://climate.onebuilding.org/WMO_Region_4_North_and_Central_America/USA_United_States_of_America/MA_Massachusetts/USA_MA_Boston-Logan.Intl.AP.725090_TMYx.2009-2023.zip"

    s3 = boto3.client("s3")
    semantic_fields_key = f"{bucket_prefix}/{experiment_id}/semantic-fields.yml"
    component_map_key = f"{bucket_prefix}/{experiment_id}/component-map.yml"
    db_key = f"{bucket_prefix}/{experiment_id}/components.db"

    semantic_fields_uri = f"s3://{bucket}/{semantic_fields_key}"
    component_map_uri = f"s3://{bucket}/{component_map_key}"
    db_uri = f"s3://{bucket}/{db_key}"

    s3.upload_file(semantic_fields_local_path.as_posix(), bucket, semantic_fields_key)
    s3.upload_file(component_map_local_path.as_posix(), bucket, component_map_key)
    s3.upload_file(db_local_path.as_posix(), bucket, db_key)

    spec = SBEMSimulationSpec(
        experiment_id=experiment_id,
        sort_index=0,
        rotated_rectangle="POLYGON ((5 0, 5 10, 15 10, 15 0, 5 0))",
        num_floors=3,
        height=3 * 3.5,
        long_edge=17,
        short_edge=17,
        aspect_ratio=1,
        rotated_rectangle_area_ratio=1,
        long_edge_angle=0.23,
        wwr=0.15,
        f2f_height=3.5,
        neighbor_polys=["POLYGON ((-10 0, -10 10, -5 10, -5 0, -10 0))"],
        neighbor_floors=[3],
        neighbor_heights=[10.5],
        epwzip_uri=AnyUrl(epwzip_path),
        db_uri=AnyUrl(db_uri),
        semantic_fields_uri=AnyUrl(semantic_fields_uri),
        component_map_uri=AnyUrl(component_map_uri),
        semantic_field_context={
            "Region": "MA",
            "Typology": "SFH",
            "Age_bracket": "post_2003",
            "AtticVentilation": "UnventilatedAttic",
            "AtticFloorInsulation": "NoInsulation",
            "RoofInsulation": "HighlyInsulatedRoof",
            "BasementCeilingInsulation": "UninsulatedCeiling",
            "BasementWallsInsulation": "UninsulatedWalls",
            "GroundSlabInsulation": "UninsulatedGroundSlab",
            "Weatherization": "TightEnvelope",
            "Walls": "FullInsulationWallsCavity",
            "Windows": "DoublePaneLowE",
            "Heating": "NaturalGasHeating",
            "Cooling": "ACWindow",
            "Distribution": "HotWaterUninsulated",
            "DHW": "NaturalGasDHW",
            "Lighting": "LED",
            "Thermostat": "Controls",
            "Equipment": "HighEfficiencyEquipment",
        },
        basement="none",
        attic="none",
    )
    specs = [
        spec.model_copy(deep=True, update={"sort_index": i}) for i in range(n_sims)
    ]
    workflow_run_dict: list[WorkflowRunDict] = [
        {
            "workflow_name": "simulate_sbem_shoebox",
            "input": spec.model_dump(mode="json"),
            "options": {},
        }
        for spec in specs
    ]
    client = new_client()
    import time

    individual_times = []
    print(f"Submitting {len(workflow_run_dict)} workflows")
    start = time.time()
    n_per_submission = 100
    for i in range(0, n_sims, n_per_submission):
        start_time = time.time()
        client.admin.run_workflows(workflow_run_dict[i : i + n_per_submission])
        end_time = time.time()
        individual_times.append(end_time - start_time)
    end = time.time()
    print(f"Sending to queue took {(end - start):.1f}s")
    print(
        f"Sending to queue per 1000 tasks took {((end - start) / n_sims * 1000):.1f}s"
    )
    print(
        f"Average time per submission: {(sum(individual_times) / len(individual_times)):.3f}s"
    )

    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(current_time)
