import logging
import re
import shutil
import tempfile
import time
from pathlib import Path

import pandas as pd
from archetypal import IDF
from archetypal.idfclass.sql import Sql
from hatchet_sdk.context import Context

from epengine.hatchet import hatchet
from epengine.models.configs import SimulationSpec, WithHContext
from epengine.models.ddy_injector import DDYSizingSpec
from epengine.utils.results import postprocess, serialize_df_dict

logger = logging.getLogger(__name__)


class SimulationSpecWithContext(WithHContext, SimulationSpec):
    pass


# TODO: This could be generated by a class method in the SimulationSpec class
# but should it?
@hatchet.workflow(
    name="simulate_epw_idf",
    on_events=["simulation:run_artifacts"],
    timeout="20m",
    version="0.3",
    schedule_timeout="1000m",
)
class Simulate:
    @hatchet.step(name="simulate", timeout="20m", retries=2)
    def simulate(self, context: Context):
        data = context.workflow_input()
        data["hcontext"] = context
        spec = SimulationSpecWithContext(**data)
        with tempfile.TemporaryDirectory() as tmpdir:
            local_pth = Path(tmpdir) / "model.idf"
            shutil.copy(spec.idf_path, local_pth)
            idf = IDF(
                local_pth,  # pyright: ignore [reportArgumentType]
                epw=spec.epw_path,
                output_directory=tmpdir,
            )  # pyright: ignore [reportArgumentType]
            if spec.ddy_path:
                # add_sizing_design_day(idf, spec.ddy_path)
                ddy = IDF(
                    spec.ddy_path,  # pyright: ignore [reportArgumentType]
                    epw=spec.epw_path,
                    as_version="9.5.0",
                    file_version="9.5.0",
                    prep_outputs=False,
                )

                ddy_spec = DDYSizingSpec(
                    design_days=[
                        "Ann Clg .4% Condns DB=>MWB",
                        "Ann Htg 99.6% Condns DB",
                    ]
                )
                ddy_spec.inject_ddy(idf, ddy)
            spec.log(f"Simulating {spec.idf_path}...")
            idf.simulate()

            time.sleep(1)
            sql = Sql(idf.sql_file)
            index_data = spec.model_dump(mode="json", exclude_none=True)
            workflow_run_id = spec.hcontext.workflow_run_id()
            # TODO: pull in spawn index
            index_data["workflow_run_id"] = workflow_run_id
            dfs = postprocess(
                sql,
                index_data=index_data,
                tabular_lookups=[("AnnualBuildingUtilityPerformanceSummary", "End Uses")],
                columns=["Electricity", "Natural Gas", "Fuel Oil No 2"],
            )

            # TODO: move these into a separate function
            end_file = idf.simulation_dir / "eplusout.end"
            err_str = end_file.read_text()
            severe_reg = r".*\s(\d+)\sSevere Errors.*"
            warning_reg = r".*\s(\d+)\sWarning.*"
            severe_matcher = re.match(severe_reg, err_str)
            warning_matcher = re.match(warning_reg, err_str)
            severe_ct = int(severe_matcher.groups()[0]) if severe_matcher else 0
            warning_ct = int(warning_matcher.groups()[0]) if warning_matcher else 0

            err_index = pd.MultiIndex.from_tuples([tuple(index_data.values())], names=list(index_data.keys()))
            err_df = pd.DataFrame({"warnings": [warning_ct], "severe": [severe_ct]}, index=err_index)
            dfs["energyplus_message_counts"] = err_df

        dfs = serialize_df_dict(dfs)

        return dfs


def add_sizing_design_day(idf: IDF, ddy_file: Path | str):
    """Read ddy file and copy objects over to self.

    Note:
        Will **NOT** add the Rain file to the model
    """
    ddy = IDF(
        ddy_file,  # pyright: ignore [reportArgumentType]
        as_version="9.2.0",
        file_version="9.2.0",
        prep_outputs=False,
    )
    for objtype, sequence in ddy.idfobjects.items():
        if sequence:
            for obj in sequence:
                if obj.key.upper() in [
                    "SITE:PRECIPITATION",
                    "ROOFIRRIGATION",
                    "SCHEDULE:FILE",
                ] and getattr(obj, "File_Name", "rain").endswith("rain"):
                    continue

                idf.removeallidfobjects(objtype)
                # idf.removeallidfobjects()
                idf.addidfobject(obj)

    del ddy
