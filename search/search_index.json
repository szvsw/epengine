{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EPEngine Hatchet","text":"<p>This is a repository for managing queues of EnergyPlus simulations with Hatchet.</p>"},{"location":"modules/","title":"Modules Reference","text":""},{"location":"modules/#models","title":"Models","text":""},{"location":"modules/#workflow-specs","title":"Workflow Specs","text":"<p>Models for Simulation Specifications.</p> <p>Models for leaf workflows.</p> <p>Models for lists and recursions of simulation specs.</p> <p>Models for workflow outputs etc.</p>"},{"location":"modules/#epengine.models.base.BaseSpec","title":"<code>BaseSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base spec for running a simulation.</p> <p>The main features are utilities to fetch files from uris and generate a locally scoped path for the files. according to the experiment_id.</p>"},{"location":"modules/#epengine.models.base.BaseSpec.fetch_uri","title":"<code>fetch_uri(uri, use_cache=True)</code>","text":"<p>Fetch a file from a uri and return the local path.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>AnyUrl</code> <p>The uri to fetch</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use the cache</p> <code>True</code> <p>Returns:</p> Name Type Description <code>local_path</code> <code>Path</code> <p>The local path of the fetched file</p>"},{"location":"modules/#epengine.models.base.BaseSpec.from_payload","title":"<code>from_payload(payload)</code>  <code>classmethod</code>","text":"<p>Create a simulation spec from a payload.</p> <p>Fetches a spec from a payload and return the spec, or validates it if it is already a spec dict.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict</code> <p>The payload to fetch</p> required <p>Returns:</p> Name Type Description <code>spec</code> <code>BaseSpec</code> <p>The fetched spec</p>"},{"location":"modules/#epengine.models.base.BaseSpec.from_uri","title":"<code>from_uri(uri)</code>  <code>classmethod</code>","text":"<p>Fetch a spec from a uri and return the spec.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>AnyUrl</code> <p>The uri to fetch</p> required <p>Returns:</p> Name Type Description <code>spec</code> <code>BaseSpec</code> <p>The fetched spec</p>"},{"location":"modules/#epengine.models.base.BaseSpec.local_path","title":"<code>local_path(pth)</code>","text":"<p>Return the local path of a uri scoped to the experiment_id.</p> <p>Note that this should only be used for non-ephemeral files.</p> <p>Parameters:</p> Name Type Description Default <code>pth</code> <code>AnyUrl</code> <p>The uri to convert to a local path</p> required <p>Returns:</p> Name Type Description <code>local_path</code> <code>Path</code> <p>The local path of the uri</p>"},{"location":"modules/#epengine.models.base.BaseSpec.log","title":"<code>log(msg)</code>","text":"<p>Log a message to the context or to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log</p> required"},{"location":"modules/#epengine.models.base.LeafSpec","title":"<code>LeafSpec</code>","text":"<p>               Bases: <code>BaseSpec</code></p> <p>A spec for running a leaf workflow.</p>"},{"location":"modules/#epengine.models.leafs.SimpleSpec","title":"<code>SimpleSpec</code>","text":"<p>               Bases: <code>LeafSpec</code></p> <p>A test spec for working with a simple leaf workflow.</p>"},{"location":"modules/#epengine.models.leafs.SimulationSpec","title":"<code>SimulationSpec</code>","text":"<p>               Bases: <code>LeafSpec</code></p> <p>A spec for running an EnergyPlus simulation.</p>"},{"location":"modules/#epengine.models.leafs.SimulationSpec.ddy_path","title":"<code>ddy_path</code>  <code>cached</code> <code>property</code>","text":"<p>Fetch the ddy file and return the local path.</p> <p>Returns:</p> Name Type Description <code>local_path</code> <code>Path</code> <p>The local path of the ddy file</p>"},{"location":"modules/#epengine.models.leafs.SimulationSpec.epw_path","title":"<code>epw_path</code>  <code>cached</code> <code>property</code>","text":"<p>Fetch the epw file and return the local path.</p> <p>Returns:</p> Name Type Description <code>local_path</code> <code>Path</code> <p>The local path of the epw file</p>"},{"location":"modules/#epengine.models.leafs.SimulationSpec.idf_path","title":"<code>idf_path</code>  <code>cached</code> <code>property</code>","text":"<p>Fetch the idf file and return the local path.</p> <p>Returns:</p> Name Type Description <code>local_path</code> <code>Path</code> <p>The local path of the idf file</p>"},{"location":"modules/#epengine.models.branches.BranchesSpec","title":"<code>BranchesSpec</code>","text":"<p>               Bases: <code>BaseSpec</code>, <code>Generic[SpecListItem]</code></p> <p>A spec for running multiple simulations.</p> <p>One key feature is that children simulations inherit the experiment_id of the parent simulations BaseSpec since they are both part of the same experiment.</p>"},{"location":"modules/#epengine.models.branches.BranchesSpec.deser_and_set_exp_id_idx","title":"<code>deser_and_set_exp_id_idx(values)</code>  <code>classmethod</code>","text":"<p>Deserializes the spec list if necessary and sets the experiment_id of each child spec to the experiment_id of the parent along with the sort_index.</p>"},{"location":"modules/#epengine.models.branches.RecursionMap","title":"<code>RecursionMap</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A map of recursion specs to use in recursive calls.</p> <p>This allows a recursion node to understand where it is in the recursion tree and how to behave.</p>"},{"location":"modules/#epengine.models.branches.RecursionMap.validate_path_is_length_ge_1","title":"<code>validate_path_is_length_ge_1(values)</code>  <code>classmethod</code>","text":"<p>Validate that the path is at least length 1.</p>"},{"location":"modules/#epengine.models.branches.RecursionSpec","title":"<code>RecursionSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A spec for recursive calls.</p>"},{"location":"modules/#epengine.models.branches.RecursionSpec.validate_offset_less_than_factor","title":"<code>validate_offset_less_than_factor(values)</code>  <code>classmethod</code>","text":"<p>Validate that the offset is less than the factor.</p>"},{"location":"modules/#epengine.models.branches.WorkflowSelector","title":"<code>WorkflowSelector</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class for generating a BranchSpec from a workflow name.</p>"},{"location":"modules/#epengine.models.branches.WorkflowSelector.BranchesSpec","title":"<code>BranchesSpec: type[BranchesSpec[LeafSpec]]</code>  <code>property</code>","text":"<p>Return the branches spec class for the workflow.</p> <p>Returns:</p> Name Type Description <code>BranchesSpecClass</code> <code>type[BranchesSpec[LeafSpec]]</code> <p>The branches spec class for the workflow</p>"},{"location":"modules/#epengine.models.branches.WorkflowSelector.Spec","title":"<code>Spec: type[LeafSpec]</code>  <code>property</code>","text":"<p>Return the spec class for the workflow.</p> <p>Returns:</p> Name Type Description <code>SpecClass</code> <code>type[LeafSpec]</code> <p>The spec class for the workflow</p>"},{"location":"modules/#epengine.models.outputs.URIResponse","title":"<code>URIResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response containing the uri of a file.</p>"},{"location":"modules/#mixins","title":"Mixins","text":"<p>Mixin Classes for working with the epengine models.</p>"},{"location":"modules/#epengine.models.mixins.WithBucket","title":"<code>WithBucket</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model with a bucket to store results.</p>"},{"location":"modules/#epengine.models.mixins.WithHContext","title":"<code>WithHContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model with a Hatchet context.</p>"},{"location":"modules/#epengine.models.mixins.WithHContext.log","title":"<code>log(msg)</code>","text":"<p>Log a message to the hatchet context.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log</p> required"},{"location":"modules/#epengine.models.mixins.WithOptionalBucket","title":"<code>WithOptionalBucket</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model with an optional bucket to store results.</p>"},{"location":"modules/#ddy-injector","title":"DDY Injector","text":"<p>A module to inject DDY files into IDF files.</p>"},{"location":"modules/#epengine.models.ddy_injector.DDYField","title":"<code>DDYField</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An enumeration of the fields in a DDY file that can be injected into an IDF file.</p>"},{"location":"modules/#epengine.models.ddy_injector.DDYFieldNotFoundError","title":"<code>DDYFieldNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a field is not found in a DDY file.</p>"},{"location":"modules/#epengine.models.ddy_injector.DDYFieldNotFoundError.__init__","title":"<code>__init__(field, obj)</code>","text":"<p>Initialize the error.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>DDYField</code> <p>The field that was not found.</p> required <code>obj</code> <code>str</code> <p>The object that was not found.</p> required"},{"location":"modules/#epengine.models.ddy_injector.DDYSizingSpec","title":"<code>DDYSizingSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class to define how to inject a DDY file into an IDF file.</p>"},{"location":"modules/#epengine.models.ddy_injector.DDYSizingSpec.handle_design_days","title":"<code>handle_design_days(idf, ddy)</code>","text":"<p>Handles the SIZINGPERIOD:DESIGNDAY field in the DDY file.</p> <p>Parameters:</p> Name Type Description Default <code>idf</code> <code>IDF</code> <p>The IDF file to inject the DDY file into.</p> required <code>ddy</code> <code>IDF</code> <p>The DDY file to inject into the IDF file.</p> required"},{"location":"modules/#epengine.models.ddy_injector.DDYSizingSpec.handle_site_location","title":"<code>handle_site_location(idf, ddy)</code>","text":"<p>Handles the SITE:LOCATION field in the DDY file.</p> <p>Parameters:</p> Name Type Description Default <code>idf</code> <code>IDF</code> <p>The IDF file to inject the DDY file into.</p> required <code>ddy</code> <code>IDF</code> <p>The DDY file to inject into the IDF file.</p> required"},{"location":"modules/#epengine.models.ddy_injector.DDYSizingSpec.handle_weather_file_condition_types","title":"<code>handle_weather_file_condition_types(idf, ddy)</code>","text":"<p>Handles the SIZINGPERIOD:WEATHERFILECONDITIONTYPE field in the DDY file.</p> <p>Parameters:</p> Name Type Description Default <code>idf</code> <code>IDF</code> <p>The IDF file to inject the DDY file into.</p> required <code>ddy</code> <code>IDF</code> <p>The DDY file to inject into the IDF.</p> required"},{"location":"modules/#epengine.models.ddy_injector.DDYSizingSpec.inject_ddy","title":"<code>inject_ddy(idf, ddy)</code>","text":"<p>Copies the DDY file into the IDF file according to the spec.</p> <p>Currently, only the following DDY fields are supported: - SITE:LOCATION - SIZINGPERIOD:DESIGNDAY - SIZINGPERIOD:WEATHERFILECONDITIONTYPE</p> <p>The following DDY fields are ignored as the just contain rain information or are not used: - RUNPERIODCONTROL:DAYLIGHTSAVINGTIME - SITE:PRECIPITATION - ROOFIRRIGATION - SCHEDULE:FILE</p> <p>Parameters:</p> Name Type Description Default <code>idf</code> <code>IDF</code> <p>The IDF file to inject the DDY file into.</p> required <code>ddy</code> <code>IDF</code> <p>The DDY file to inject into the IDF file.</p> required"},{"location":"modules/#epengine.models.ddy_injector.DDYSizingSpec.remove_and_replace","title":"<code>remove_and_replace(idf, ddy, field, copy_names)</code>","text":"<p>Removes all objects of the given field and replaces them with the new ones.</p> <p>Raises an error if the object is not found in the DDY file and <code>self.raise_on_not_found</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>idf</code> <code>IDF</code> <p>The IDF file to remove and replace objects from.</p> required <code>ddy</code> <code>IDF</code> <p>The DDY file to copy objects from.</p> required <code>field</code> <code>DDYField</code> <p>The field to remove and replace objects from.</p> required <code>copy_names</code> <code>set[WeatherFileConditionType] | set[DesignDayName]</code> <p>The names of the objects to copy.</p> required"},{"location":"modules/#utils","title":"Utils","text":""},{"location":"modules/#results","title":"Results","text":"<p>This module contains functions to postprocess and serialize results.</p>"},{"location":"modules/#epengine.utils.results.CombineRecurseResultsMultipleKeysError","title":"<code>CombineRecurseResultsMultipleKeysError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a result dictionary contains more than one key when it has a \"uri\" key.</p>"},{"location":"modules/#epengine.utils.results.CombineRecurseResultsMultipleKeysError.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the error.</p>"},{"location":"modules/#epengine.utils.results.collate_subdictionaries","title":"<code>collate_subdictionaries(results)</code>","text":"<p>Collate subdictionaries into a single dictionary of dataframes.</p> <p>Note that this assumes the dictionaries are in the tight orientation and that the index keys are the same across all dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[dict[str, dict]]</code> <p>A list of dictionaries of dataframes</p> required <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, pd.DataFrame]: A dictionary of dataframes</p>"},{"location":"modules/#epengine.utils.results.combine_recurse_results","title":"<code>combine_recurse_results(results)</code>","text":"<p>Combines the results of recursive operations into a single dictionary of DataFrames.</p> <p>The recursive returns may have been uris or explicit results. This function combines them into a single dictionary of DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[dict[str, Any]]</code> <p>A list of dictionaries representing the results of recursive operations.</p> required <p>Returns:</p> Name Type Description <code>collected_dfs</code> <code>dict[str, DataFrame]</code> <p>A dictionary containing the combined DataFrames, where the keys are the URIs of the DataFrames.</p> <p>Raises:</p> Type Description <code>CombineRecurseResultsMultipleKeysError</code> <p>If a result dictionary contains more than one key when it has a \"uri\" key.</p>"},{"location":"modules/#epengine.utils.results.create_errored_and_missing_df","title":"<code>create_errored_and_missing_df(errored_workflows, missing_results)</code>","text":"<p>Creates a DataFrame of errored and missing results.</p> <p>Parameters:</p> Name Type Description Default <code>errored_workflows</code> <code>list[tuple[str, ZipDataContent, BaseException]]</code> <p>The list of errored workflows.</p> required <code>missing_results</code> <code>list[tuple[str, ZipDataContent]]</code> <p>The list of missing results.</p> required <p>Returns:</p> Name Type Description <code>errors</code> <code>DataFrame</code> <p>The DataFrame of errored and missing results.</p>"},{"location":"modules/#epengine.utils.results.handle_explicit_result","title":"<code>handle_explicit_result(collected_dfs, result)</code>","text":"<p>Updates the collected dataframes with an explicit result.</p> <p>Note that this function mutates the collected_dfs dictionary and does not return a value.</p> <p>Parameters:</p> Name Type Description Default <code>collected_dfs</code> <code>dict[str, DataFrame]</code> <p>A dictionary containing the collected dataframes.</p> required <code>result</code> <code>dict</code> <p>The explicit result to handle.</p> required"},{"location":"modules/#epengine.utils.results.handle_referenced_result","title":"<code>handle_referenced_result(collected_dfs, uri)</code>","text":"<p>Fetches a result from a given URI and updates the collected dataframes.</p> <p>Note that this function mutates the collected_dfs dictionary and does not return a value.</p> <p>Parameters:</p> Name Type Description Default <code>collected_dfs</code> <code>dict[str, DataFrame]</code> <p>A dictionary containing the collected dataframes.</p> required <code>uri</code> <code>str</code> <p>The URI of the result to fetch.</p> required"},{"location":"modules/#epengine.utils.results.postprocess","title":"<code>postprocess(sql, index_data, tabular_lookups, columns=None)</code>","text":"<p>Postprocess tabular data from the SQL file.</p> <p>Requests a series of Energy Plus table lookups and return the data in a dictionary of dataframes with a single row; the provided index data is configured as the MultiIndex of the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>Sql</code> <p>The sql object to query</p> required <code>index_data</code> <code>dict</code> <p>The index data to use</p> required <code>tabular_lookups</code> <code>list[tuple[str, str]]</code> <p>The tabular data to query</p> required <code>columns</code> <code>list[str]</code> <p>The columns to keep. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, pd.DataFrame]: A dictionary of dataframes</p>"},{"location":"modules/#epengine.utils.results.save_and_upload_results","title":"<code>save_and_upload_results(collected_dfs, s3, bucket, output_key, save_errors=False)</code>","text":"<p>Saves and uploads the collected dataframes to S3.</p> <p>Parameters:</p> Name Type Description Default <code>collected_dfs</code> <code>dict[str, DataFrame]</code> <p>A dictionary containing the collected dataframes.</p> required <code>bucket</code> <code>str</code> <p>The S3 bucket to upload the results to.</p> required <code>output_key</code> <code>str</code> <p>The key to use for the uploaded results.</p> required <code>save_errors</code> <code>bool</code> <p>Whether to save errors in the results.</p> <code>False</code> <code>s3</code> <code>S3Client</code> <p>The S3 client to use for uploading the results.</p> required <p>Returns:</p> Name Type Description <code>uri</code> <code>str</code> <p>The URI of the uploaded results.</p>"},{"location":"modules/#epengine.utils.results.separate_errors_and_safe_sim_results","title":"<code>separate_errors_and_safe_sim_results(ids, zip_data, results)</code>","text":"<p>Separates errored workflows from safe simulation results.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>The list of workflow IDs.</p> required <code>zip_data</code> <code>list[ZipDataContent]</code> <p>The list of data to return with rows.</p> required <code>results</code> <code>list[ResultDataContent | BaseException]</code> <p>The list of results to separate.</p> required <p>Returns:</p> Name Type Description <code>safe_results</code> <code>list[tuple[str, ZipDataContent, ResultDataContent]]]</code> <p>A list of safe results.</p> <code>errored_results</code> <code>list[tuple[str, ZipDataContent, BaseException]]</code> <p>A list of errored results.</p>"},{"location":"modules/#epengine.utils.results.serialize_df_dict","title":"<code>serialize_df_dict(dfs)</code>","text":"<p>Serialize a dictionary of dataframes into a dictionary of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>dict[str, DataFrame]</code> <p>A dictionary of dataframes</p> required <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: A dictionary of dictionaries</p>"},{"location":"modules/#epengine.utils.results.update_collected_with_df","title":"<code>update_collected_with_df(collected_dfs, key, df)</code>","text":"<p>Updates the collected dataframes with a new DataFrame.</p> <p>Note that this function mutates the collected_dfs dictionary and does not return a value.</p> <p>Parameters:</p> Name Type Description Default <code>collected_dfs</code> <code>dict[str, DataFrame]</code> <p>A dictionary containing the collected dataframes.</p> required <code>key</code> <code>str</code> <p>The key to use for the new DataFrame.</p> required <code>df</code> <code>DataFrame</code> <p>The DataFrame to add to the collected dataframes.</p> required"},{"location":"modules/#filesystem-utilities","title":"FileSystem Utilities","text":"<p>Filesystem utilities.</p>"},{"location":"modules/#epengine.utils.filesys.fetch_uri","title":"<code>fetch_uri(uri, local_path, use_cache=True, logger_fn=logger.info, s3=s3)</code>","text":"<p>Fetch a file from a uri and return the local path.</p> <p>Caching is enabled by default and works by checking if the file exists locally before downloading it to avoid downloading the same file multiple times.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>AnyUrl</code> <p>The uri to fetch</p> required <code>local_path</code> <code>Path</code> <p>The local path to save the fetched file</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use the cache</p> <code>True</code> <code>logger_fn</code> <code>Callable</code> <p>The logger function to use</p> <code>info</code> <code>s3</code> <code>S3Client</code> <p>The S3 client to use</p> <code>s3</code> <p>Returns:</p> Name Type Description <code>local_path</code> <code>Path</code> <p>The local path of the fetched file</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>The following pages will walk you through setting up a distributed EnergyPlus simulation engine.</p> <p>For local/development purposes, see local instructions.</p> <p>For deploying to AWS, see AWS instructions.</p>"},{"location":"getting-started/copilot/","title":"Copilot Deployment","text":"<p>WARNING: THESE DOCS ARE SLIGHTLY OUT OF DATE. STAY TUNED FOR AN UPDATE!</p>"},{"location":"getting-started/copilot/#first-time-setup","title":"First Time Setup","text":"<p>Copilot is an official AWS tool used to quickly stand-up infrastructure in the cloud. For this project, it is responsible for setting up a backend service for running simulations (so no inbound internet-access, only outbound) and an S3 bucket. In the future, it will also include the distributed queuing system and the API which is used to submit tasks, however, for now those are expected to be running with managed hosting (for the queue) and locally (for the API, if needed).</p> <p>Once the prereqs are installed, you can either run a single command macro to stand up the whole stack at once, or you can stand-up pieces incrementally.</p>"},{"location":"getting-started/copilot/#prereqs","title":"Prereqs","text":"<ol> <li>Install Docker.</li> <li>Install AWS CLI &amp; AWS Copilot.</li> <li>Create a Hatchet account.</li> <li>Generate a Hatchet API token. nb: in the future, Hatchet will be deployable as part of the Copilot specification.</li> <li>Configure an AWS CLI profile if you have not done so already: <code>aws configure --profile &lt;your-profile-name&gt;</code></li> <li>Clone the repository: <code>git clone https://github.com/szvsw/epengine</code>.</li> <li>Move into the repository: <code>cd epengine</code></li> </ol>"},{"location":"getting-started/copilot/#command-macro-whole-stack-at-once","title":"Command macro (whole stack at once)","text":"<p>Running the following command will walk you through all the steps necessary to stand-up the application in AWS.</p> <pre><code>make create-copilot-app\n</code></pre> <p>You will be prompted to select a few things:</p> <ol> <li>A name for the application.</li> <li>The AWS profile to use.</li> <li>Any configuration necessary for the environment (e.g. security groups, VPCs, subnets etc), however you can use the defaults if desired (recommended for most users).</li> <li>The Hatchet Client Token</li> </ol> <p>Once complete, if all is susccessful, you should see one worker in your Hatchet pool.</p>"},{"location":"getting-started/copilot/#manually-running-the-commands","title":"Manually running the commands","text":"<ol> <li><code>copilot app init</code></li> <li>see <code>app init</code> help for more details like <code>--permissions-boundary</code> if needed</li> <li><code>copilot env init --name prod</code></li> <li>(see <code>see init</code> help for more details, as well as <code>init</code> manifest help for specs for VPCs etc.)</li> <li><code>copilot env deploy --name prod</code></li> <li><code>copilot secret init --name HATCHET_CLIENT_TOKEN</code></li> <li><code>copilot deploy --init-wkld --env prod --all</code></li> </ol> <p>Once complete, if all is susccessful, you should see one worker in your Hatchet pool.</p>"},{"location":"getting-started/copilot/#managing-worker-counts","title":"Managing Worker Counts","text":"<p>In <code>copilot/worker/manifest.yml</code>, you can edit the worker count configuration to determine how many workers you would like, if scaling should be done based off of CPU utilization, etc.</p> <p>After making changes, run <code>copilot svc deploy</code>.</p>"},{"location":"getting-started/copilot/#cleanup","title":"Cleanup","text":"<ol> <li>Empty your buckets if they have files in them.</li> <li><code>copilot app delete</code></li> </ol>"},{"location":"getting-started/hatchet-deploy/","title":"sequence","text":"<ol> <li>copilot env init --name prod</li> <li>consider turning on rabbit mq console public accessibilty, setting instance sizes for rabbit mq</li> <li>copilot env deploy --name prod</li> <li>copilot svc init --name hatchet-engine</li> <li>copilot svc deploy --name hatchet-engine --env prod</li> <li>update the the volume mount fs-id and fsap-id</li> <li>update the alias and import the certificate arn</li> <li>copilot svc init --name hatchet-api</li> <li>copilot svc deploy --name hatchet-api --env prod</li> <li>update your cname record to point at the load balancer</li> <li>log in</li> <li>make a tenant</li> <li>or login with <code>admin@example.com</code> / <code>Admin123!!</code></li> <li>make an api token</li> <li>(these steps can potentially be automated with a task run)</li> <li>copilot secret init --name HATCHET_CLIENT_TOKEN --overwrite</li> <li>disable/enable TLS</li> <li>copilot svc init --name hatchet-worker</li> <li>make docker-login</li> <li>copilot svc deploy --name hatchet-worker --env prod</li> </ol>"},{"location":"getting-started/local/","title":"Local Development","text":""},{"location":"getting-started/local/#installation","title":"Installation","text":"<ul> <li>Docker</li> <li>poetry</li> </ul> <pre><code>make install\n</code></pre>"},{"location":"getting-started/local/#configuration","title":"Configuration","text":"<pre><code>cp .env.example .env\ncp .env.example .env.dev\n</code></pre> <p>You will then want to enter your AWS credentials into <code>.env</code> and your Hatchet credentials into both.</p>"},{"location":"getting-started/local/#running-the-system","title":"Running the System","text":"<pre><code>make dev\n</code></pre> <pre><code>make prod\n</code></pre>"}]}